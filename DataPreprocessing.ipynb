{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emgg7.csv')\n",
    "keep_col = ['emg1','emg2','emg3','emg4','emg5','emg6','emg7','emg8']\n",
    "new_df = df[keep_col]\n",
    "new_df = new_df[:-256]\n",
    "trimmed_df = new_df.iloc[256:]\n",
    "\n",
    "#trimmed_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Create x, where x the 'scores' column's values as floats\n",
    "x = trimmed_df.values.astype(float)\n",
    "\n",
    "# Create a minimum and maximum processor object\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# Create an object to transform the data to fit minmax processor\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "\n",
    "# Run the normalizer on the dataframe\n",
    "df_normalized = pd.DataFrame(x_scaled)\n",
    "#df_normalized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For EMG Values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Compute Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.030377722546781474,\n",
       " 0.019183611677776668,\n",
       " 0.02394703009680093,\n",
       " 0.007313511426457975,\n",
       " 0.016827837773521696,\n",
       " 0.02978541056961171,\n",
       " 0.03282829823946298,\n",
       " 0.007428605452804537,\n",
       " 0.03052900661759214,\n",
       " 0.011279896194410027,\n",
       " 0.005299680693804103,\n",
       " 0.04926719318224238,\n",
       " 0.05887300831944484,\n",
       " 0.025576407233898972,\n",
       " 0.03374054145185335,\n",
       " 0.02503432549391946,\n",
       " 0.037446726548245726,\n",
       " 0.021991260335940104,\n",
       " 0.01772972419153922,\n",
       " 0.024892155860968258,\n",
       " 0.037301714126149875,\n",
       " 0.028544989803436283,\n",
       " 0.01422835887709492,\n",
       " 0.021929871413926577,\n",
       " 0.01824954512099919,\n",
       " 0.019252518521023932,\n",
       " 0.013433596095099068,\n",
       " 0.017997047614575355,\n",
       " 0.020736702758794513,\n",
       " 0.035909375015111676,\n",
       " 0.025663729592750335,\n",
       " 0.07053917325970224,\n",
       " 0.025666537521864787,\n",
       " 0.0202690790004382,\n",
       " 0.01615129388924645,\n",
       " 0.02064357404445394,\n",
       " 0.020080194838860345,\n",
       " 0.010145698334073014,\n",
       " 0.047355984403338984,\n",
       " 0.06675367033659148,\n",
       " 0.02803838282395024,\n",
       " 0.036626703785645455,\n",
       " 0.05565324993265706,\n",
       " 0.04433328676938171,\n",
       " 0.04843370687902695,\n",
       " 0.03794218250156163,\n",
       " 0.04232049953710058,\n",
       " 0.02033966410221367,\n",
       " 0.03315961863427404,\n",
       " 0.02143938594713069,\n",
       " 0.006991693591607964,\n",
       " 0.011850097220910476,\n",
       " 0.05080650659963629,\n",
       " 0.02846114377959796,\n",
       " 0.02168490173674213,\n",
       " 0.022534144732305066,\n",
       " 0.01511956962096223,\n",
       " 0.021476853397410206,\n",
       " 0.06631392101355785,\n",
       " 0.02514965041817772,\n",
       " 0.023866436497384205,\n",
       " 0.03990558397466283,\n",
       " 0.05766316080630028,\n",
       " 0.02088241663881251,\n",
       " 0.00834516177853368,\n",
       " 0.022976071937520968,\n",
       " 0.024534850808027694,\n",
       " 0.020843520063347352,\n",
       " 0.020783941882701273,\n",
       " 0.01586330451462841,\n",
       " 0.013863120333585234,\n",
       " 0.042558038085897344,\n",
       " 0.026830933980284796,\n",
       " 0.03791615527864979,\n",
       " 0.023398006048809927,\n",
       " 0.005131504435514455,\n",
       " 0.006168236817852291,\n",
       " 0.042523955030327985,\n",
       " 0.028166752019674945,\n",
       " 0.01494339837164849,\n",
       " 0.009766836524571812,\n",
       " 0.026270845239112956,\n",
       " 0.0144215058060571,\n",
       " 0.01671564835355082,\n",
       " 0.032649021092335785,\n",
       " 0.006486674285286131,\n",
       " 0.0550498193750326,\n",
       " 0.03688728936391388,\n",
       " 0.015231880411646043,\n",
       " 0.03258215773865621,\n",
       " 0.010843141877760079,\n",
       " 0.00910780471820718,\n",
       " 0.021216155218867244,\n",
       " 0.02082400667402325,\n",
       " 0.034119398416209916,\n",
       " 0.02736816288781817,\n",
       " 0.015144246667507574,\n",
       " 0.02032289125244328,\n",
       " 0.034477790597171015,\n",
       " 0.01835872534752859,\n",
       " 0.018919276071147323,\n",
       " 0.06003715883460494,\n",
       " 0.016736214224384753,\n",
       " 0.04295049023723101,\n",
       " 0.022243310276750687,\n",
       " 0.011105115261891382,\n",
       " 0.033853483558044226,\n",
       " 0.03673181503076344,\n",
       " 0.026234110568849188,\n",
       " 0.08089010853940416,\n",
       " 0.02343782091557122,\n",
       " 0.03577434852876788,\n",
       " 0.015668975320207647,\n",
       " 0.04910970372979998,\n",
       " 0.02366729556240642,\n",
       " 0.021092797278449895,\n",
       " 0.0428773164044546,\n",
       " 0.027893302156985295,\n",
       " 0.07080320481626153,\n",
       " 0.009992915639206495,\n",
       " 0.01801407160904537,\n",
       " 0.04723086847453543,\n",
       " 0.024945078887090747,\n",
       " 0.015204767102195633,\n",
       " 0.0203131206325689,\n",
       " 0.040620691884764124,\n",
       " 0.028682477595389177,\n",
       " 0.029737480677078088,\n",
       " 0.04060493364432637,\n",
       " 0.010197383859572293,\n",
       " 0.04429956351395327,\n",
       " 0.029224190543116698,\n",
       " 0.07027914282261367,\n",
       " 0.032766486257547425,\n",
       " 0.014729408106362793,\n",
       " 0.009738891788800321,\n",
       " 0.034762040261358866,\n",
       " 0.005322045182690165,\n",
       " 0.00988265117083212,\n",
       " 0.039889128930013625,\n",
       " 0.011370408435011092,\n",
       " 0.029390712920818823,\n",
       " 0.028412887287492222,\n",
       " 0.047781979642707174,\n",
       " 0.030635963803021356,\n",
       " 0.02766437464654514,\n",
       " 0.02189087677434024,\n",
       " 0.056532803807770864,\n",
       " 0.04981358947899796,\n",
       " 0.025159772348295587,\n",
       " 0.01999321091871926,\n",
       " 0.03991637332930454,\n",
       " 0.012900031589982874,\n",
       " 0.03265793604778912,\n",
       " 0.027585751281245564,\n",
       " 0.014710594903272938,\n",
       " 0.03648263311075866,\n",
       " 0.030800385413003624,\n",
       " 0.04286649666327441,\n",
       " 0.008849683261276378,\n",
       " 0.017645466666023207,\n",
       " 0.011996981289161915,\n",
       " 0.05669349453799035,\n",
       " 0.029296363195174337,\n",
       " 0.03477341850628777,\n",
       " 0.02303892351081316,\n",
       " 0.01816934570994583,\n",
       " 0.026465282693118455,\n",
       " 0.03371622011553464,\n",
       " 0.021188545820042953,\n",
       " 0.026219598625812784,\n",
       " 0.02982793270204049,\n",
       " 0.012808029082521719,\n",
       " 0.014175621893659552,\n",
       " 0.024836589914302053,\n",
       " 0.0347520199573605,\n",
       " 0.013932320406241858,\n",
       " 0.014829111857585129,\n",
       " 0.05510810620094974,\n",
       " 0.031035477300967864,\n",
       " 0.07310588491197632,\n",
       " 0.03859505535469504,\n",
       " 0.019860929657817255,\n",
       " 0.051839104440389354,\n",
       " 0.015893092597634096,\n",
       " 0.03827856787706512,\n",
       " 0.03486741430042963,\n",
       " 0.02220679089033072,\n",
       " 0.020995705357014588,\n",
       " 0.018683750971212488,\n",
       " 0.015454729795184346,\n",
       " 0.02580409662831625,\n",
       " 0.014918477676065146,\n",
       " 0.015140399249934308,\n",
       " 0.029533803255962773,\n",
       " 0.06682180242968283,\n",
       " 0.044260553585022185,\n",
       " 0.013186491900584537,\n",
       " 0.01902839723053844,\n",
       " 0.043029427197190835,\n",
       " 0.07082595077654451,\n",
       " 0.04726088025552518,\n",
       " 0.03454972236125844,\n",
       " 0.03676799839405579,\n",
       " 0.028666746609044967,\n",
       " 0.054143546187911056,\n",
       " 0.029620140365978605,\n",
       " 0.020099374252574874,\n",
       " 0.028741220198720763,\n",
       " 0.016734022299672968,\n",
       " 0.02790807551441372,\n",
       " 0.028122813538668252,\n",
       " 0.023530381281451303,\n",
       " 0.017377696304425427,\n",
       " 0.013745231785850678,\n",
       " 0.0066455618893597425,\n",
       " 0.026375460664560757,\n",
       " 0.02136667420553193,\n",
       " 0.014728725346741188,\n",
       " 0.01895056017368773,\n",
       " 0.012965726753259176,\n",
       " 0.005957550100972869,\n",
       " 0.038072987196030826,\n",
       " 0.03301662807742277,\n",
       " 0.017526121470814362,\n",
       " 0.04393178098211476,\n",
       " 0.023497195295000228,\n",
       " 0.02571972413288951,\n",
       " 0.013460495389051494,\n",
       " 0.02741676541337717,\n",
       " 0.01908316905743875,\n",
       " 0.05489668428203576,\n",
       " 0.027969065373601092,\n",
       " 0.018753315633550077,\n",
       " 0.011322200631463806,\n",
       " 0.019392948123030045,\n",
       " 0.04911782915566939,\n",
       " 0.025323032331498522,\n",
       " 0.021329656325218892,\n",
       " 0.0056122305345813975,\n",
       " 0.038031527988366856,\n",
       " 0.032797540912080626,\n",
       " 0.01571006092296039,\n",
       " 0.017818165069386917,\n",
       " 0.01053509360041732,\n",
       " 0.01771997340749389,\n",
       " 0.008684404822632873,\n",
       " 0.034634405364844786,\n",
       " 0.019624857847003886,\n",
       " 0.02039585203378092,\n",
       " 0.008142785274627888,\n",
       " 0.02629615071906081,\n",
       " 0.03048961650913442,\n",
       " 0.01792926037413939,\n",
       " 0.016269219452396785,\n",
       " 0.009964150618382763]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#varEmg1 = df_normalized.var()[0]\n",
    "#varEmg2 = df_normalized.var()[1]\n",
    "#varEmg3 = df_normalized.var()[2]\n",
    "#varEmg4 = df_normalized.var()[3]\n",
    "#varEmg5 = df_normalized.var()[4]\n",
    "#varEmg6 = df_normalized.var()[5]\n",
    "#varEmg7 = df_normalized.var()[6]\n",
    "#varEmg8 = df_normalized.var()[7]\n",
    "\n",
    "#df_normalized.var(axis=1).tolist()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Compute Mean Absolute Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-eb5ceca765d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmavValues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mrows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_normalized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'length' is not defined"
     ]
    }
   ],
   "source": [
    "from statsmodels import robust\n",
    "\n",
    "rows = []\n",
    "mavValues = []\n",
    "\n",
    "for x in range(0, length):\n",
    "    rows.append(df_normalized.iloc[x].tolist())\n",
    "\n",
    "#mavEmg1 = robust.mad(df_normalized)[0]\n",
    "#mavEmg2 = robust.mad(df_normalized)[1]\n",
    "#mavEmg3 = robust.mad(df_normalized)[2]\n",
    "#mavEmg4 = robust.mad(df_normalized)[3]\n",
    "#mavEmg5 = robust.mad(df_normalized)[4]\n",
    "#mavEmg6 = robust.mad(df_normalized)[5]\n",
    "#mavEmg7 = robust.mad(df_normalized)[6]\n",
    "#mavEmg8 = robust.mad(df_normalized)[7]\n",
    "\n",
    "for x in range(0, length):\n",
    "    mavValues.append(robust.mad(rows[x]))\n",
    "    \n",
    "#mavValues"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Compute Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 25,  64, 124, 256, 425, 431, 399, 221,  73,  30], dtype=int64),\n",
       " array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.histogram(df_normalized)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Root Mean Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.537600044099375,\n",
       " 0.6077977904739744,\n",
       " 0.6284437435177315,\n",
       " 0.5287435693496212,\n",
       " 0.5481970122933211,\n",
       " 0.39902074911162644,\n",
       " 0.5128712200486903,\n",
       " 0.505954637214357,\n",
       " 0.4513434462621655,\n",
       " 0.5354410975754279,\n",
       " 0.44992058392195466,\n",
       " 0.4680338458733236,\n",
       " 0.626143942628735,\n",
       " 0.5968184977540655,\n",
       " 0.5573440004389347,\n",
       " 0.6833559606206459,\n",
       " 0.5312705304863911,\n",
       " 0.5982314625530978,\n",
       " 0.387547722586453,\n",
       " 0.5154413572959413,\n",
       " 0.5162158706982596,\n",
       " 0.36354588006804983,\n",
       " 0.41657844689803547,\n",
       " 0.5097161758570636,\n",
       " 0.7212869547327103,\n",
       " 0.5290703301139288,\n",
       " 0.5621707682460972,\n",
       " 0.5011262515392166,\n",
       " 0.5524479652025319,\n",
       " 0.5056863624462319,\n",
       " 0.6183135168006033,\n",
       " 0.4959996109524417,\n",
       " 0.42937997907764286,\n",
       " 0.41423141660005386,\n",
       " 0.5341487042624976,\n",
       " 0.48927607034482934,\n",
       " 0.6871304525161706,\n",
       " 0.4921676480440291,\n",
       " 0.4799380309843155,\n",
       " 0.477200471714975,\n",
       " 0.5179856467570374,\n",
       " 0.5627627026341724,\n",
       " 0.6753482905441202,\n",
       " 0.683287882661414,\n",
       " 0.37901418164764317,\n",
       " 0.4563017133518443,\n",
       " 0.4309658021699087,\n",
       " 0.5033269608660891,\n",
       " 0.6393326718496729,\n",
       " 0.40216287900520353,\n",
       " 0.6067961087090314,\n",
       " 0.7223833484258421,\n",
       " 0.4916465381788577,\n",
       " 0.46283170305694704,\n",
       " 0.4380315125471285,\n",
       " 0.45762536712631524,\n",
       " 0.6197682879894117,\n",
       " 0.7391374429840109,\n",
       " 0.539256866688846,\n",
       " 0.5000752288148415,\n",
       " 0.29811857767692124,\n",
       " 0.44936141770342286,\n",
       " 0.4554580872074594,\n",
       " 0.6198272786213712,\n",
       " 0.6153854461455178,\n",
       " 0.5939395575598694,\n",
       " 0.453528026817336,\n",
       " 0.4953181223903057,\n",
       " 0.47851043953174,\n",
       " 0.6028613111327888,\n",
       " 0.5862553606010275,\n",
       " 0.41152317195194377,\n",
       " 0.45379144669290317,\n",
       " 0.522251325108527,\n",
       " 0.5053202665777786,\n",
       " 0.43749143585424827,\n",
       " 0.5931092360970824,\n",
       " 0.4724791405695839,\n",
       " 0.5937881655381604,\n",
       " 0.5637233614755813,\n",
       " 0.6247295059768045,\n",
       " 0.4445418898585084,\n",
       " 0.5068097787330705,\n",
       " 0.5082859860152124,\n",
       " 0.5112960416559815,\n",
       " 0.6775698505109005,\n",
       " 0.5337067518797924,\n",
       " 0.38431337853677394,\n",
       " 0.43370048193829447,\n",
       " 0.5567013892058208,\n",
       " 0.5528333395020133,\n",
       " 0.6384854973734239,\n",
       " 0.6394321520887886,\n",
       " 0.41872491102208476,\n",
       " 0.4195120070061438,\n",
       " 0.4972068468082269,\n",
       " 0.6415417390727323,\n",
       " 0.6961368845436361,\n",
       " 0.5055500729461734,\n",
       " 0.579603275019358,\n",
       " 0.38163910352635405,\n",
       " 0.4804139580024842,\n",
       " 0.3249123001565005,\n",
       " 0.5669613299904492,\n",
       " 0.8087565872182476,\n",
       " 0.5979439646603121,\n",
       " 0.5056563491293501,\n",
       " 0.3206347954542612,\n",
       " 0.6167409761350433,\n",
       " 0.5433840265682819,\n",
       " 0.5216005997730383,\n",
       " 0.4624582650243968,\n",
       " 0.3833608708718129,\n",
       " 0.5055716295955468,\n",
       " 0.6502015148104824,\n",
       " 0.5792336643232759,\n",
       " 0.3659317163853637,\n",
       " 0.5743180574241307,\n",
       " 0.5995573957822968,\n",
       " 0.6962442075252001,\n",
       " 0.485546386306597,\n",
       " 0.41447171477445777,\n",
       " 0.4594679204093614,\n",
       " 0.5035109403016436,\n",
       " 0.5224237918340707,\n",
       " 0.5375970175385101,\n",
       " 0.6200327323039907,\n",
       " 0.6488600300262588,\n",
       " 0.3552701304325871,\n",
       " 0.5824200337037004,\n",
       " 0.5947411841078821,\n",
       " 0.5435307292508451,\n",
       " 0.4007078737743356,\n",
       " 0.47196178251704035,\n",
       " 0.5363507896487142,\n",
       " 0.48552069231950973,\n",
       " 0.6887551252146193,\n",
       " 0.4838486083593058,\n",
       " 0.5635404486031266,\n",
       " 0.5285502533076217,\n",
       " 0.619496926913524,\n",
       " 0.592958091491265,\n",
       " 0.45609740767687157,\n",
       " 0.5116319804040317,\n",
       " 0.510035199633196,\n",
       " 0.5178050518064915,\n",
       " 0.49907625781070786,\n",
       " 0.42132150150000897,\n",
       " 0.5146596996378601,\n",
       " 0.6312322362947533,\n",
       " 0.44638948732259365,\n",
       " 0.6027207095296812,\n",
       " 0.5477012405459267,\n",
       " 0.4919320031621144,\n",
       " 0.5591844583838222,\n",
       " 0.6454388141538261,\n",
       " 0.47989008725946125,\n",
       " 0.32618915117857117,\n",
       " 0.3936511592842947,\n",
       " 0.5699210037592783,\n",
       " 0.5307227570313144,\n",
       " 0.6677091836687357,\n",
       " 0.5945383489449417,\n",
       " 0.42323229927087425,\n",
       " 0.39492278707618517,\n",
       " 0.547199742412752,\n",
       " 0.5778905022202121,\n",
       " 0.533590442658319,\n",
       " 0.6280829665511155,\n",
       " 0.5366305005407652,\n",
       " 0.33082234901506513,\n",
       " 0.5527198314366119,\n",
       " 0.6260767753438741,\n",
       " 0.5049333443443157,\n",
       " 0.6377423315715205,\n",
       " 0.46660452472106595,\n",
       " 0.5789105688247931,\n",
       " 0.534825049123478,\n",
       " 0.4640059804621187,\n",
       " 0.5737661729262441,\n",
       " 0.5060801537444857,\n",
       " 0.6190979281794462,\n",
       " 0.46070261309046046,\n",
       " 0.5320771179076145,\n",
       " 0.5231159769019489,\n",
       " 0.5117643665039527,\n",
       " 0.6333930951721893,\n",
       " 0.541700071590819,\n",
       " 0.40144871944277216,\n",
       " 0.5255479588776846,\n",
       " 0.5774379910926823,\n",
       " 0.5044733337360249,\n",
       " 0.6796372264421677,\n",
       " 0.4504493127790965,\n",
       " 0.4223855011252018,\n",
       " 0.48221406070543377,\n",
       " 0.5227007318353103,\n",
       " 0.5139711798183928,\n",
       " 0.680831504360811,\n",
       " 0.4431449957908371,\n",
       " 0.5568677011302013,\n",
       " 0.4173720106796846,\n",
       " 0.44457597002186466,\n",
       " 0.5380266389869302,\n",
       " 0.5718086650794547,\n",
       " 0.6538748897745486,\n",
       " 0.5196650505219629,\n",
       " 0.43207082033034655,\n",
       " 0.5366837705335128,\n",
       " 0.567736146968305,\n",
       " 0.47910124447532576,\n",
       " 0.5230822232133346,\n",
       " 0.4255373021451615,\n",
       " 0.5984743699307744,\n",
       " 0.5284803066514073,\n",
       " 0.6067669994840877,\n",
       " 0.34356832429407175,\n",
       " 0.48985760596906214,\n",
       " 0.6328758274278392,\n",
       " 0.5931724419066372,\n",
       " 0.6860153834160048,\n",
       " 0.3579913856076471,\n",
       " 0.5482082116063864,\n",
       " 0.5407491145589852,\n",
       " 0.6142098031641361,\n",
       " 0.4960615023999007,\n",
       " 0.45199816293770234,\n",
       " 0.49947570209539077,\n",
       " 0.4770113994441022,\n",
       " 0.6227905736686555,\n",
       " 0.6107917517353829,\n",
       " 0.405827605923619,\n",
       " 0.4617631985226273,\n",
       " 0.5485604353283168,\n",
       " 0.6677551675351354,\n",
       " 0.5385957259767082,\n",
       " 0.4785672388404344,\n",
       " 0.4358216157982101,\n",
       " 0.5111089120038579,\n",
       " 0.6048544136771639,\n",
       " 0.49978655071621214,\n",
       " 0.5426046268414179,\n",
       " 0.4760659432539449,\n",
       " 0.41706203882848036,\n",
       " 0.6215137894809172,\n",
       " 0.5658302617124064,\n",
       " 0.6179606492018017,\n",
       " 0.3029051766225149,\n",
       " 0.47945208358771463,\n",
       " 0.3947577201197332,\n",
       " 0.548882202429146,\n",
       " 0.6372367443892205,\n",
       " 0.49994266347422706,\n",
       " 0.5891657208894697,\n",
       " 0.5189318222260092,\n",
       " 0.5905791236292575]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rmsEmg1 = np.sqrt(np.mean(df_normalized[0].values**2))\n",
    "#rmsEmg2 = np.sqrt(np.mean(df_normalized[1].values**2))\n",
    "#rmsEmg3 = np.sqrt(np.mean(df_normalized[2].values**2))\n",
    "#rmsEmg4 = np.sqrt(np.mean(df_normalized[3].values**2))\n",
    "#rmsEmg5 = np.sqrt(np.mean(df_normalized[4].values**2))\n",
    "#rmsEmg6 = np.sqrt(np.mean(df_normalized[5].values**2))\n",
    "#rmsEmg7 = np.sqrt(np.mean(df_normalized[6].values**2))\n",
    "#rmsEmg8 = np.sqrt(np.mean(df_normalized[7].values**2))\n",
    "\n",
    "np.sqrt(df_normalized.mean(axis=1)**2).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trimming (For Acceleration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('accelerometer-time.csv')\n",
    "keep_col = ['x', 'y', 'z']\n",
    "new_df = df[keep_col]\n",
    "#new_df = new_df[:-256]\n",
    "#trimmed_df = new_df.iloc[256:]\n",
    "trimmed_df = new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization (For Acceleration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Create x, where x the 'scores' column's values as floats\n",
    "x = trimmed_df.values.astype(float)\n",
    "\n",
    "# Create a minimum and maximum processor object\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# Create an object to transform the data to fit minmax processor\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "\n",
    "# Run the normalizer on the dataframe\n",
    "df_normalized = pd.DataFrame(x_scaled)\n",
    "#df_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance (For Acceleration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0886223903962441,\n",
       " 0.08622825432610715,\n",
       " 0.08157637135555976,\n",
       " 0.07811528165245345,\n",
       " 0.08008807458957348,\n",
       " 0.07910757637174821,\n",
       " 0.07919833743058656,\n",
       " 0.07878287856383773,\n",
       " 0.08160545211760356,\n",
       " 0.08281487447365625,\n",
       " 0.08324069202780214,\n",
       " 0.080268733923642,\n",
       " 0.08236484198047136,\n",
       " 0.08239542552583648,\n",
       " 0.08334451528196919,\n",
       " 0.08203856646550622,\n",
       " 0.07829140289055127,\n",
       " 0.08094017464933823,\n",
       " 0.08092016897004445,\n",
       " 0.08283708873786705,\n",
       " 0.08159739811453773,\n",
       " 0.08205197721146552,\n",
       " 0.07739190717754435,\n",
       " 0.07194838222145826,\n",
       " 0.05581882490019464,\n",
       " 0.06570082978365353,\n",
       " 0.09628224535400902,\n",
       " 0.16754000270509045,\n",
       " 0.14577929445167837,\n",
       " 0.08524662539392072,\n",
       " 0.059691219668956574,\n",
       " 0.0726524680154215,\n",
       " 0.059360924504026844,\n",
       " 0.03816509831039292,\n",
       " 0.02949637064713312,\n",
       " 0.028159397709873173,\n",
       " 0.027762921981012538,\n",
       " 0.013642016169913437,\n",
       " 0.011242269084313541,\n",
       " 0.031090322181305554,\n",
       " 0.05738819744201444,\n",
       " 0.05950191344605332,\n",
       " 0.05818952362964719,\n",
       " 0.08786077045425111,\n",
       " 0.08671320965444984,\n",
       " 0.08189857509249976,\n",
       " 0.07934559369477642,\n",
       " 0.061692365317539195,\n",
       " 0.060205331276811554,\n",
       " 0.037677866045559186,\n",
       " 0.017648350809828878,\n",
       " 0.004531290151247281,\n",
       " 0.011980533404167526,\n",
       " 0.03842167483959929,\n",
       " 0.02821921987886493,\n",
       " 0.01544316404060571,\n",
       " 0.004358412730253334,\n",
       " 0.015217635833223577,\n",
       " 0.031029090434195786,\n",
       " 0.06771283300454711,\n",
       " 0.0948569219046715,\n",
       " 0.10183414459869614,\n",
       " 0.09968285268669255,\n",
       " 0.0603471691893562,\n",
       " 0.08506966518609721,\n",
       " 0.15845678483374573,\n",
       " 0.2624810558004922,\n",
       " 0.01617282167790719,\n",
       " 0.005730161702878364,\n",
       " 0.025128123763236462,\n",
       " 0.042728596927076944,\n",
       " 0.08416918508909496,\n",
       " 0.1051920428842537,\n",
       " 0.08349295521898394,\n",
       " 0.07838850474014429,\n",
       " 0.13966901471646187,\n",
       " 0.13780925796033594,\n",
       " 0.11504381309301161,\n",
       " 0.006932702608860395,\n",
       " 0.009992687437319294,\n",
       " 0.02695630979521258,\n",
       " 0.05013779625898049,\n",
       " 0.04880068040264081,\n",
       " 0.037717858026964035,\n",
       " 0.022723250925395646,\n",
       " 0.012820616761004598,\n",
       " 0.0062455760562488945,\n",
       " 0.004371252482486944,\n",
       " 0.003477604867498737,\n",
       " 0.0031473885099991567,\n",
       " 0.0053287745947522315,\n",
       " 0.013334517924865646,\n",
       " 0.012862007797694145,\n",
       " 0.00881068654431338,\n",
       " 0.005288660805269027,\n",
       " 0.010539966002091809,\n",
       " 0.010500540807094524,\n",
       " 0.013950642867606463,\n",
       " 0.012223346375089175,\n",
       " 0.006695892594748647,\n",
       " 0.0007013779640377949,\n",
       " 0.011200614718908914,\n",
       " 0.017443955740531827,\n",
       " 0.02219168382947238,\n",
       " 0.02992165095146499,\n",
       " 0.024607427977100478,\n",
       " 0.025876705479859426,\n",
       " 0.018279130829190308,\n",
       " 0.030846352819351993,\n",
       " 0.05277666020315649,\n",
       " 0.05254465738608495,\n",
       " 0.032352425472360787,\n",
       " 0.03431585734163889,\n",
       " 0.04663796364699059,\n",
       " 0.0665998794279284,\n",
       " 0.12010206294983014,\n",
       " 0.13612371701140988,\n",
       " 0.05623206722242214,\n",
       " 0.02932559368780973,\n",
       " 0.04850061177736686,\n",
       " 0.09671917433992801,\n",
       " 0.11607081718436424,\n",
       " 0.0959085521128906,\n",
       " 0.09873327865374971,\n",
       " 0.1002066844079415,\n",
       " 0.09133383028273016,\n",
       " 0.08071967586450599,\n",
       " 0.08346028366744629,\n",
       " 0.08931744060011569,\n",
       " 0.09450905034701963,\n",
       " 0.08942688788344583,\n",
       " 0.07601757516930574,\n",
       " 0.07316537052946226,\n",
       " 0.07041298532643493,\n",
       " 0.07688063037404423,\n",
       " 0.08332812458868556,\n",
       " 0.08879895796108596,\n",
       " 0.09541590075679206,\n",
       " 0.09927857371097083,\n",
       " 0.0966969399179124,\n",
       " 0.09888881012929915,\n",
       " 0.09830090828764458,\n",
       " 0.09331761917434725,\n",
       " 0.09594049695925906,\n",
       " 0.08954791090187836,\n",
       " 0.08158117330186207,\n",
       " 0.08156880937966651,\n",
       " 0.08387944466500034,\n",
       " 0.08104519320820447,\n",
       " 0.08554304809374945,\n",
       " 0.08667997360719515,\n",
       " 0.08412048768966691,\n",
       " 0.08480964685310233,\n",
       " 0.08186669939841412,\n",
       " 0.08679114262484514,\n",
       " 0.08353063205311013,\n",
       " 0.08560279124128956,\n",
       " 0.0860915161485443,\n",
       " 0.08783685837808486,\n",
       " 0.08658037907227342,\n",
       " 0.09223401463213265,\n",
       " 0.08749971307130558,\n",
       " 0.08673648707702544,\n",
       " 0.08910990298410007,\n",
       " 0.08650271319334951,\n",
       " 0.08728804079345824,\n",
       " 0.08352519030609776,\n",
       " 0.0875894532608237,\n",
       " 0.08564626841617276,\n",
       " 0.08350175047720344,\n",
       " 0.08322338996814636,\n",
       " 0.08515062383148571,\n",
       " 0.08607182990066384,\n",
       " 0.08710105607329678,\n",
       " 0.08759655510431691,\n",
       " 0.08680088883904308,\n",
       " 0.08975681036990114,\n",
       " 0.08550853924891413,\n",
       " 0.08260434662392604,\n",
       " 0.08526735896151506,\n",
       " 0.08518296982645483,\n",
       " 0.08445780058653063,\n",
       " 0.08319298048393431,\n",
       " 0.08389288635361979,\n",
       " 0.08395446226162404,\n",
       " 0.08660459537047467,\n",
       " 0.08419192259757469,\n",
       " 0.08701042574364053,\n",
       " 0.0866119264101509,\n",
       " 0.08738272491471269,\n",
       " 0.0837838648677573,\n",
       " 0.0846127497744255,\n",
       " 0.08303940051147943,\n",
       " 0.08383662435805968,\n",
       " 0.08341491964095639,\n",
       " 0.08860853749390571,\n",
       " 0.08656195490480939,\n",
       " 0.08581936796003775,\n",
       " 0.08591273617415927,\n",
       " 0.08363396833782812,\n",
       " 0.08574769187890907,\n",
       " 0.08589948442119165,\n",
       " 0.08698814947373758,\n",
       " 0.0881861384306846,\n",
       " 0.08540946822411324,\n",
       " 0.08396800541169257,\n",
       " 0.08650258042848585,\n",
       " 0.0852955627702384,\n",
       " 0.08537610977064945,\n",
       " 0.08433847788711088,\n",
       " 0.08627550900631901]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_normalized.var(axis=1).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Absolute Value (For Acceleration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels import robust\n",
    "\n",
    "rows = []\n",
    "mavValues = []\n",
    "length = df_normalized.shape[0]\n",
    "\n",
    "for x in range(0, length):\n",
    "    rows.append(df_normalized.iloc[x].tolist())\n",
    "\n",
    "for x in range(0, length):\n",
    "    mavValues.append(robust.mad(rows[x]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root Mean Square (For Acceleration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4593563578661642,\n",
       " 0.4581589902826211,\n",
       " 0.4594387353392449,\n",
       " 0.45360747496507714,\n",
       " 0.45889408004797166,\n",
       " 0.45141250400610816,\n",
       " 0.4535773480693413,\n",
       " 0.4503880880137157,\n",
       " 0.44649308881882166,\n",
       " 0.45305330033639923,\n",
       " 0.44426606567328203,\n",
       " 0.44958951875050895,\n",
       " 0.44169946329230925,\n",
       " 0.44530207264292937,\n",
       " 0.4506697091321503,\n",
       " 0.4525461336548804,\n",
       " 0.4593107697260988,\n",
       " 0.45812780205340253,\n",
       " 0.45507625773246624,\n",
       " 0.45429252450774665,\n",
       " 0.45167007545940624,\n",
       " 0.4553600785107809,\n",
       " 0.46022550588483974,\n",
       " 0.47654779307788314,\n",
       " 0.5278949203273605,\n",
       " 0.5761987197590496,\n",
       " 0.5540620503732462,\n",
       " 0.5375374920928891,\n",
       " 0.5317357703997555,\n",
       " 0.4706265969332373,\n",
       " 0.3690885369770293,\n",
       " 0.3029547070983365,\n",
       " 0.2784336816826644,\n",
       " 0.2766852616154771,\n",
       " 0.27988668334733013,\n",
       " 0.26949087214051165,\n",
       " 0.2419982459043003,\n",
       " 0.20262671851347402,\n",
       " 0.19173657415008302,\n",
       " 0.22259672751268997,\n",
       " 0.21188362509949096,\n",
       " 0.239106069800154,\n",
       " 0.24813427196160376,\n",
       " 0.26066035772662427,\n",
       " 0.2784003319723171,\n",
       " 0.29983159846759966,\n",
       " 0.3510024305909845,\n",
       " 0.3962908718988323,\n",
       " 0.42115016356078055,\n",
       " 0.4308043130688423,\n",
       " 0.48546366130716256,\n",
       " 0.5417514495922255,\n",
       " 0.579751613013653,\n",
       " 0.6105256376959445,\n",
       " 0.5581744549008529,\n",
       " 0.5095965083132502,\n",
       " 0.5382932843226698,\n",
       " 0.538514797850517,\n",
       " 0.5297428316464465,\n",
       " 0.4803104874866196,\n",
       " 0.4823098629604204,\n",
       " 0.48912133433342553,\n",
       " 0.5507533881806937,\n",
       " 0.5853263621239885,\n",
       " 0.5704472753392872,\n",
       " 0.5380759789989679,\n",
       " 0.4354992098226898,\n",
       " 0.5476398758885631,\n",
       " 0.5540305803661495,\n",
       " 0.5558931421586638,\n",
       " 0.6176246545043148,\n",
       " 0.5572585132821174,\n",
       " 0.5358353864893137,\n",
       " 0.5781749375736048,\n",
       " 0.6377273165425144,\n",
       " 0.6322290069559356,\n",
       " 0.5223226479039585,\n",
       " 0.5284070891331224,\n",
       " 0.5848184375235758,\n",
       " 0.5217056988829291,\n",
       " 0.5437815138500645,\n",
       " 0.5310029513911899,\n",
       " 0.5280590865163116,\n",
       " 0.5186844539197553,\n",
       " 0.4895153055561457,\n",
       " 0.5059616804584467,\n",
       " 0.5432692361595326,\n",
       " 0.5510389369945042,\n",
       " 0.5418369379440947,\n",
       " 0.5014858088161391,\n",
       " 0.4656167429471041,\n",
       " 0.4511072137474847,\n",
       " 0.4182721385771859,\n",
       " 0.39218111153116664,\n",
       " 0.3781880948349096,\n",
       " 0.3537977259721015,\n",
       " 0.3144041195749858,\n",
       " 0.26473270774966334,\n",
       " 0.2834176114865276,\n",
       " 0.2858026475591858,\n",
       " 0.27253533040827266,\n",
       " 0.29854925882357636,\n",
       " 0.38548785767741034,\n",
       " 0.41235272423483255,\n",
       " 0.42346843431278636,\n",
       " 0.4155919203637013,\n",
       " 0.44353324350624534,\n",
       " 0.4630815853397452,\n",
       " 0.43049751111987583,\n",
       " 0.4447746405728858,\n",
       " 0.4332776649851125,\n",
       " 0.4610119327079006,\n",
       " 0.4385157102536213,\n",
       " 0.43529061868201224,\n",
       " 0.4763041158218995,\n",
       " 0.46953872155710524,\n",
       " 0.47753690865015885,\n",
       " 0.527973138085299,\n",
       " 0.5007291609603105,\n",
       " 0.46844344200093474,\n",
       " 0.45652382775164435,\n",
       " 0.4571191118108579,\n",
       " 0.4898929402181658,\n",
       " 0.5007994012555187,\n",
       " 0.4768990568170817,\n",
       " 0.46562165566970487,\n",
       " 0.46478633844886197,\n",
       " 0.46129030763082773,\n",
       " 0.4684680805418879,\n",
       " 0.47832567814163135,\n",
       " 0.47992180131632406,\n",
       " 0.4820906663798859,\n",
       " 0.4763075366777015,\n",
       " 0.48123796036558725,\n",
       " 0.47941822780542204,\n",
       " 0.47604086335248413,\n",
       " 0.47234034154411436,\n",
       " 0.4652848853678637,\n",
       " 0.46620161584298286,\n",
       " 0.46797008508838916,\n",
       " 0.45984184768849806,\n",
       " 0.46190504448914527,\n",
       " 0.4625292420235505,\n",
       " 0.4638045397123544,\n",
       " 0.4658295190775761,\n",
       " 0.46682663270939767,\n",
       " 0.4634080448418689,\n",
       " 0.46828116191495534,\n",
       " 0.47071811247401435,\n",
       " 0.4718050085446726,\n",
       " 0.473856109214881,\n",
       " 0.47453658629171885,\n",
       " 0.47056677163591115,\n",
       " 0.4754837794229608,\n",
       " 0.468188616854846,\n",
       " 0.47440154402838314,\n",
       " 0.47393772309786314,\n",
       " 0.47481378129333723,\n",
       " 0.47978232204939514,\n",
       " 0.4739092050874231,\n",
       " 0.46755425936761413,\n",
       " 0.4658118157747371,\n",
       " 0.4697628554299969,\n",
       " 0.46562822716671076,\n",
       " 0.4732029921366809,\n",
       " 0.471565015626759,\n",
       " 0.475057091550602,\n",
       " 0.47558958932009493,\n",
       " 0.474009937365542,\n",
       " 0.4789951974144278,\n",
       " 0.4780211034559701,\n",
       " 0.47140449267385853,\n",
       " 0.47506429150602547,\n",
       " 0.47377558296173833,\n",
       " 0.47308983122031184,\n",
       " 0.47461415254745815,\n",
       " 0.4707824865014048,\n",
       " 0.4708569618203051,\n",
       " 0.4734279552231093,\n",
       " 0.47175643411084706,\n",
       " 0.4742055267821066,\n",
       " 0.4771869708738438,\n",
       " 0.47287532045418373,\n",
       " 0.47561275534247444,\n",
       " 0.4733000355917252,\n",
       " 0.4719834688371259,\n",
       " 0.474087091201149,\n",
       " 0.46762231100074464,\n",
       " 0.47468015916765266,\n",
       " 0.4707922135791666,\n",
       " 0.4743544127759903,\n",
       " 0.4722257918318044,\n",
       " 0.47642993784763404,\n",
       " 0.47908224772059366,\n",
       " 0.4745178726825124,\n",
       " 0.4701251754469464,\n",
       " 0.4697339818679638,\n",
       " 0.4662266574855014,\n",
       " 0.46886011503444114,\n",
       " 0.46942114622395376,\n",
       " 0.47107068236942057,\n",
       " 0.4680897434308673,\n",
       " 0.47342364152474975,\n",
       " 0.47391279354324406,\n",
       " 0.4753467151904705,\n",
       " 0.4722583261277773,\n",
       " 0.4724008344176532,\n",
       " 0.47238043459967893,\n",
       " 0.4674319272232251,\n",
       " 0.46987692628547983,\n",
       " 0.46897871237399374]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.sqrt(df_normalized.mean(axis=1)**2).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[ 0.375       0.49305556  0.40972222  0.52083333  0.59722222  0.27083333\n  0.71527778  0.38888889  0.13888889  0.45833333  0.46527778  0.5\n  0.77083333  0.35416667  0.31944444  0.68055556  0.15277778  0.59722222\n  0.52083333  0.38888889  0.49305556  0.38888889  0.23611111  0.22916667\n  0.65277778  0.65972222  0.49305556  0.42361111  0.44444444  0.25694444\n  0.34722222  0.73611111  0.10416667  0.46527778  0.52083333  0.34722222\n  0.63888889  0.36111111  0.59027778  0.09722222  0.57638889  0.40277778\n  0.20138889  0.54861111  0.47222222  0.46527778  0.61805556  0.55555556\n  0.45138889  0.25        0.59027778  0.59027778  0.45138889  0.45833333\n  0.33333333  0.54166667  0.50694444  0.66666667  0.50694444  0.43055556\n  0.125       0.5         0.30555556  0.33333333  0.5625      0.47916667\n  0.34722222  0.35416667  0.375       0.39583333  0.52777778  0.44444444\n  0.13888889  0.64583333  0.36805556  0.47222222  0.64583333  0.13888889\n  0.57638889  0.53472222  0.50694444  0.34027778  0.57638889  0.51388889\n  0.29861111  0.75        0.16666667  0.34027778  0.42361111  0.65972222\n  0.33333333  0.65972222  0.4375      0.47222222  0.40972222  0.15277778\n  0.48611111  0.53472222  0.40972222  0.57638889  0.52777778  0.34722222\n  0.54166667  0.23611111  0.5625      0.45138889  0.625       0.33333333\n  0.76388889  0.09722222  0.59027778  0.25        0.42361111  0.42361111\n  0.77083333  0.40277778  0.27777778  0.50694444  0.29861111  0.79861111\n  0.375       0.22222222  0.36805556  0.48611111  0.48611111  0.1875\n  0.65972222  0.35416667  0.47222222  0.54861111  0.52083333  0.36805556\n  0.44444444  0.48611111  0.4375      0.47222222  0.51388889  0.56944444\n  0.45833333  0.50694444  0.76388889  0.47222222  0.20138889  0.31944444\n  0.4375      0.59722222  0.40277778  0.5625      0.10416667  0.40277778\n  0.29166667  0.52083333  0.48611111  0.52777778  0.30555556  0.63194444\n  0.34027778  0.40277778  0.41666667  0.47222222  0.48611111  0.6875\n  0.29166667  0.67361111  0.20138889  0.47222222  0.38194444  0.32638889\n  0.36111111  0.47222222  0.22222222  0.71527778  0.59722222  0.34027778\n  0.50694444  0.29861111  0.47222222  0.46527778  0.66666667  0.46527778\n  0.          0.66666667  0.35416667  0.70833333  0.32638889  0.36111111\n  0.50694444  0.36805556  0.27777778  0.36805556  0.52777778  0.42361111\n  0.81944444  0.40277778  0.18055556  0.34027778  0.52083333  0.39583333\n  0.67361111  0.35416667  0.88194444  0.15972222  0.36805556  0.33333333\n  0.49305556  1.          0.48611111  0.36111111  0.375       0.44444444\n  0.11111111  0.25        0.38194444  0.72916667  0.52777778  0.64583333\n  0.20138889  0.3125      0.50694444  0.54861111  0.80555556  0.41666667\n  0.4375      0.22222222  0.40972222  0.08333333  0.72222222  0.61111111\n  0.50694444  0.40277778  0.49305556  0.36805556  0.40972222  0.42361111\n  0.59027778  0.63194444  0.38888889  0.22916667  0.20138889  0.58333333\n  0.20833333  0.64583333  0.25        0.22916667  0.63888889  0.40277778\n  0.73611111  0.14583333  0.45138889  0.33333333  0.47916667  0.61805556\n  0.30555556  0.31944444  0.55555556  0.54166667].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-2370a9c6cb28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# fit a SVM model to the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_normalized\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_normalized\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# make predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparse\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[0;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 573\u001b[1;33m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[0;32m    574\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    439\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    442\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m             \u001b[1;31m# To ensure that array flags are maintained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[ 0.375       0.49305556  0.40972222  0.52083333  0.59722222  0.27083333\n  0.71527778  0.38888889  0.13888889  0.45833333  0.46527778  0.5\n  0.77083333  0.35416667  0.31944444  0.68055556  0.15277778  0.59722222\n  0.52083333  0.38888889  0.49305556  0.38888889  0.23611111  0.22916667\n  0.65277778  0.65972222  0.49305556  0.42361111  0.44444444  0.25694444\n  0.34722222  0.73611111  0.10416667  0.46527778  0.52083333  0.34722222\n  0.63888889  0.36111111  0.59027778  0.09722222  0.57638889  0.40277778\n  0.20138889  0.54861111  0.47222222  0.46527778  0.61805556  0.55555556\n  0.45138889  0.25        0.59027778  0.59027778  0.45138889  0.45833333\n  0.33333333  0.54166667  0.50694444  0.66666667  0.50694444  0.43055556\n  0.125       0.5         0.30555556  0.33333333  0.5625      0.47916667\n  0.34722222  0.35416667  0.375       0.39583333  0.52777778  0.44444444\n  0.13888889  0.64583333  0.36805556  0.47222222  0.64583333  0.13888889\n  0.57638889  0.53472222  0.50694444  0.34027778  0.57638889  0.51388889\n  0.29861111  0.75        0.16666667  0.34027778  0.42361111  0.65972222\n  0.33333333  0.65972222  0.4375      0.47222222  0.40972222  0.15277778\n  0.48611111  0.53472222  0.40972222  0.57638889  0.52777778  0.34722222\n  0.54166667  0.23611111  0.5625      0.45138889  0.625       0.33333333\n  0.76388889  0.09722222  0.59027778  0.25        0.42361111  0.42361111\n  0.77083333  0.40277778  0.27777778  0.50694444  0.29861111  0.79861111\n  0.375       0.22222222  0.36805556  0.48611111  0.48611111  0.1875\n  0.65972222  0.35416667  0.47222222  0.54861111  0.52083333  0.36805556\n  0.44444444  0.48611111  0.4375      0.47222222  0.51388889  0.56944444\n  0.45833333  0.50694444  0.76388889  0.47222222  0.20138889  0.31944444\n  0.4375      0.59722222  0.40277778  0.5625      0.10416667  0.40277778\n  0.29166667  0.52083333  0.48611111  0.52777778  0.30555556  0.63194444\n  0.34027778  0.40277778  0.41666667  0.47222222  0.48611111  0.6875\n  0.29166667  0.67361111  0.20138889  0.47222222  0.38194444  0.32638889\n  0.36111111  0.47222222  0.22222222  0.71527778  0.59722222  0.34027778\n  0.50694444  0.29861111  0.47222222  0.46527778  0.66666667  0.46527778\n  0.          0.66666667  0.35416667  0.70833333  0.32638889  0.36111111\n  0.50694444  0.36805556  0.27777778  0.36805556  0.52777778  0.42361111\n  0.81944444  0.40277778  0.18055556  0.34027778  0.52083333  0.39583333\n  0.67361111  0.35416667  0.88194444  0.15972222  0.36805556  0.33333333\n  0.49305556  1.          0.48611111  0.36111111  0.375       0.44444444\n  0.11111111  0.25        0.38194444  0.72916667  0.52777778  0.64583333\n  0.20138889  0.3125      0.50694444  0.54861111  0.80555556  0.41666667\n  0.4375      0.22222222  0.40972222  0.08333333  0.72222222  0.61111111\n  0.50694444  0.40277778  0.49305556  0.36805556  0.40972222  0.42361111\n  0.59027778  0.63194444  0.38888889  0.22916667  0.20138889  0.58333333\n  0.20833333  0.64583333  0.25        0.22916667  0.63888889  0.40277778\n  0.73611111  0.14583333  0.45138889  0.33333333  0.47916667  0.61805556\n  0.30555556  0.31944444  0.55555556  0.54166667].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# Support Vector Machine\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "# load the iris datasets\n",
    "dataset = datasets.load_iris()\n",
    "# fit a SVM model to the data\n",
    "model = SVC()\n",
    "model.fit(dataset.data, dataset.target)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = dataset.target\n",
    "predicted = model.predict(dataset.data)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
